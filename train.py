import torch
import torch.optim as optim
import torch.nn as nn
import argparse
import os
from dataloader import create_dataloaders
from models.unetgenerator import UNetGenerator
from models.patchgandiscriminator import PatchGANDiscriminator

# Argument parser for flexible inputs
def parse_args():
    parser = argparse.ArgumentParser(description="Train a GAN for sketch-to-image generation.")
    parser.add_argument('--sketch_dir', type=str, required=True, help='Path to the sketches directory.')
    parser.add_argument('--photo_dir', type=str, required=True, help='Path to the photos directory.')
    parser.add_argument('--batch_size', type=int, default=16, help='Batch size for training.')
    parser.add_argument('--num_epochs', type=int, default=50, help='Number of epochs to train.')
    parser.add_argument('--lr', type=float, default=0.0002, help='Learning rate for optimizer.')
    parser.add_argument('--save_path', type=str, default='./models', help='Path to save the trained models.')
    parser.add_argument('--patience', type=int, default=10, help='Patience for early stopping.')
    return parser.parse_args()

# Main training function
def train(args):
    # Set up device: use GPU if available, otherwise CPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Create dataloaders
    train_loader, val_loader, test_loader = create_dataloaders(args.sketch_dir, args.photo_dir, args.batch_size)

    # Initialize models
    generator = UNetGenerator().to(device)
    discriminator = PatchGANDiscriminator().to(device)

    # Loss functions
    criterion_GAN = nn.BCEWithLogitsLoss()  # For adversarial loss
    criterion_L1 = nn.L1Loss()  # For pixel-level similarity

    # Optimizers
    optimizer_G = optim.Adam(generator.parameters(), lr=args.lr, betas=(0.5, 0.999))
    optimizer_D = optim.Adam(discriminator.parameters(), lr=args.lr, betas=(0.5, 0.999))

    # Training settings
    best_val_loss = float("inf")
    patience_counter = 0

    # Create directory to save models if not exists
    os.makedirs(args.save_path, exist_ok=True)

    # Training loop
    for epoch in range(args.num_epochs):
        generator.train()
        discriminator.train()

        total_g_loss = 0
        total_d_loss = 0

        for i, (sketches, real_images) in enumerate(train_loader):
            sketches = sketches.to(device)
            real_images = real_images.to(device)

            # === Train Discriminator ===
            optimizer_D.zero_grad()

            # Real images
            real_output = discriminator(real_images, sketches)
            real_labels = torch.ones_like(real_output).to(device)
            loss_real = criterion_GAN(real_output, real_labels)

            # Fake images (generated by the generator)
            fake_images = generator(sketches)
            fake_output = discriminator(fake_images.detach(), sketches)
            fake_labels = torch.zeros_like(fake_output).to(device)
            loss_fake = criterion_GAN(fake_output, fake_labels)

            # Total loss for discriminator
            loss_D = (loss_real + loss_fake) * 0.5
            loss_D.backward()
            optimizer_D.step()

            # === Train Generator ===
            optimizer_G.zero_grad()

            # Adversarial loss (tries to fool the discriminator)
            fake_output = discriminator(fake_images, sketches)
            loss_G_GAN = criterion_GAN(fake_output, real_labels)

            # L1 loss (ensures pixel-level similarity between generated and real images)
            loss_G_L1 = criterion_L1(fake_images, real_images) * 100  # L1 loss scaled by 100

            # Total loss for generator
            loss_G = loss_G_GAN + loss_G_L1
            loss_G.backward()
            optimizer_G.step()

            total_g_loss += loss_G.item()
            total_d_loss += loss_D.item()

            print(f"[Epoch {epoch}/{args.num_epochs}] [Batch {i}/{len(train_loader)}] "
                  f"[D loss: {loss_D.item():.4f}] [G loss: {loss_G.item():.4f}]")

        # === Validation Phase ===
        generator.eval()
        val_loss = 0.0
        with torch.no_grad():
            for val_sketches, val_real_images in val_loader:
                val_sketches = val_sketches.to(device)
                val_real_images = val_real_images.to(device)

                # Generate fake images
                val_fake_images = generator(val_sketches)

                # Calculate validation L1 loss
                val_loss += criterion_L1(val_fake_images, val_real_images).item()

        val_loss /= len(val_loader)
        print(f"Validation Loss: {val_loss:.4f}")

        # Early Stopping and Model Saving
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            # Save the best model
            torch.save(generator.state_dict(), os.path.join(args.save_path, "best_generator.pth"))
            torch.save(discriminator.state_dict(), os.path.join(args.save_path, "best_discriminator.pth"))
            print(f"Model saved at epoch {epoch} with validation loss: {val_loss:.4f}")
        else:
            patience_counter += 1
            if patience_counter >= args.patience:
                print(f"Early stopping triggered at epoch {epoch}")
                break

if __name__ == "__main__":
    args = parse_args()
    train(args)